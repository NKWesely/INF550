---
title: "INF 550 Section 5.7"
author: "Natasha Wesely"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(neonUtilities)
# install.packages('BiocManager')
library(BiocManager)
# BiocManager::install('rhdf5')
library(rhdf5)
library(eddy4R.base)
library(jsonlite)
library(lubridate)

options(stringsAsFactors=F)

# Ameriflux site that relates to JORN
# US-xJR
```

# Flux Measurements & Inter-Operability Coding Exercise
## Question 1
**NEON data are submitted to AmeriFlux quarterly after one year of non-quality flagged or otherwise missing data are available. Use the workflow above to extend the data coverage of an already submitted NEON site by downloading existing data from the AmeriFlux website and recently published HDF5 files from the NEON data portal. Process the NEON data such that it is in AmeriFlux format and plot the entire timerseries.**

```{r}

# # download NEON data for JORN site
# zipsByProduct(dpID="DP4.00200.001", 
#               package="basic", 
#               site="JORN", 
#               startdate="2018-06", 
#               enddate="2018-07",
#               savepath="./data_5.7", 
#               check.size=F)
# 
# # download recently published HDF5 filse from NEON data portal 
# 
# 
# flux <- stackEddy(filepath="./data_5.7/filesToStack00200",
#                   level="dp04")
# 
# # fix the dates
# timeB <- as.POSIXct(flux$NIWO$timeBgn, 
#                     format="%Y-%m-%dT%H:%M:%S", 
#                     tz="GMT")
# flux$NIWO <- cbind(timeB, flux$NIWO)
# 
# 
# # download another data product 
# pr <- loadByProduct("DP1.00024.001", site="NIWO", avg=30,
#                     startdate="2018-06", enddate="2018-07",
#                     package="basic", check.size=F)

```

```{r}
# #Install most  recent version of ffbase from GitHub, this is a package dependency of eddy4R.base
# devtools::install_github("edwindj/ffbase", subdir="pkg")
# 
# #Install NEONprocIS.base from GitHub, this package is a dependency of eddy4R.base
# devtools::install_github(repo="NEONScience/NEON-IS-data-processing",
#                          ref="master",
#                          subdir="pack/NEONprocIS.base",
#                          dependencies=c(NA, TRUE)[2],
#                          repos=c(BiocManager::repositories(),   # for dependencies on Bioconductor packages
#                                  "https://cran.rstudio.com/")       # for CRAN
# )
# 
# #Install eddy4R.base from GitHub
# devtools::install_github(repo="NEONScience/eddy4R",
#                          ref="master",
#                          subdir="pack/eddy4R.base",
#                          dependencies=c(NA, TRUE)[2],
#                          repos=c(BiocManager::repositories(),   # for dependencies on Bioconductor packages
#                                  "https://cran.rstudio.com/")       # for CRAN
# )

# # make a list of the required packages
# packReq <- c("rhdf5", "eddy4R.base", "jsonlite", "lubridate")
# 
# # load the required packages
# lapply(packReq, function(x) {
#   print(x)
#   if(require(x, character.only = TRUE) == FALSE) {
#     install.packages(x)
#     library(x, character.only = TRUE)
#   }})

# disable file locking 
h5disableFileLocking()

# choose your NEON site of interest
site <- "JORN"

#define start and end dates, optional, defaults to entire period of site operation. Use %Y-%m-%d format.
dateBgn <- "2021-01-01"
dateEnd <- "2021-12-31"

# Data package from the portal
Pack <- c('basic','expanded')[1]
#The version data for the FP standard conversion processing
ver = paste0("v",format(Sys.time(), "%Y%m%dT%H%m"))


# Specify Download directory for HDF5 files from the NEON data portal and output directory to save the resulting csv files. Change save paths to where you want the files on your computer.
#download directory
DirDnld=tempdir()

#Output directory, change this to where you want to save the output csv
DirOutBase <-paste0("~/data_5.7/eddy/data/Ameriflux/",ver)

# Specify Data Product number, for the Bundled Eddy-Covariance files, this is DP4.00200.001
#DP number
dpID <- 'DP4.00200.001'


# Get metadata from Ameriflux Site Info BADM sheets for the site of interest

#Grab a list of all Ameriflux sites, containing site ID and site description
sites_web <- jsonlite::fromJSON("http://ameriflux-data.lbl.gov/AmeriFlux/SiteSearch.svc/SiteList/AmeriFlux")

#Grab only your neon site of interest NEON site from the AmeriFlux sites
siteNeon <- sites_web[grep(pattern = paste0("NEON.*",site), x = sites_web$SITE_NAME),]

# get the AmeriFlux site info for your site of interest
metaSite <- lapply(siteNeon$SITE_ID, function(x) {
  pathSite <- paste0("http://ameriflux-data.lbl.gov/BADM/Anc/SiteInfo/",x)
  tmp <- fromJSON(pathSite)
  return(tmp)
}) 


# Use Ameriflux site IDs to name metadata lists
#use NEON ID as list name
names(metaSite) <- site 

# Check if dateBgn is defined, if not make it the initial operations date “IOCR” of the site
if(!exists("dateBgn") || is.na(dateBgn) || is.null(dateBgn)){
  dateBgn <- as.Date(metaSite[[site]]$values$GRP_FLUX_MEASUREMENTS[[1]]$FLUX_MEASUREMENTS_DATE_START, "%Y%m%d")
} else {
  dateBgn <- dateBgn
}#End of checks for missing dateBgn

#Check if dateEnd is defined, if not make it the system date
if(!exists("dateEnd") || is.na(dateEnd) || is.null(dateEnd)){
  dateEnd <- as.Date(Sys.Date())
} else {
  dateEnd <- dateEnd
}#End of checks for missing dateEnd


# Grab the UTC time offset from the Ameriflux API
timeOfstUtc <- as.integer(metaSite[[site]]$values$GRP_UTC_OFFSET[[1]]$UTC_OFFSET)

# Create the date sequence
setDate <- seq(from = as.Date(dateBgn), to = as.Date(dateEnd), by = "month")

# Start processing the site time range specified, verify that the site and date range are specified as intended
msg <- paste0("Starting Ameriflux FP standard conversion processing workflow for ", site, " for ", dateBgn, " to ", dateEnd)
print(msg)


# Create output directory by checking if the download directory exists and create it if not
if(dir.exists(DirDnld) == FALSE) dir.create(DirDnld, recursive = TRUE)
#Append the site to the base output directory
DirOut <- paste0(DirOutBase, "/", siteNeon$SITE_ID)
#Check if directory exists and create if not
if(!dir.exists(DirOut)) dir.create(DirOut, recursive = TRUE)

# Download and extract data

#Initialize data List
dataList <- list()

#Read data from the API
dataList <- lapply(setDate, function(x) {
  date <- stringr::str_extract(x, pattern = paste0("[0-9]{4}", "-", "[0-9]{2}"))
  tryCatch(neonUtilities::zipsByProduct(dpID = dpID, site = site, startdate = date, enddate = date, package =      "basic", savepath = DirDnld, check.size = FALSE), error=function(e) NULL)
  files <- list.files(paste0(DirDnld, "/filesToStack00200"))
  utils::unzip(paste0(DirDnld, "/filesToStack00200/", files[grep(pattern = paste0(site,".*.", date, ".*.zip"),     x = files)]), exdir = paste0(DirDnld, "/filesToStack00200"))
  files <- list.files(paste0(DirDnld, "/filesToStack00200"))
  R.utils::gunzip(paste0(DirDnld, "/filesToStack00200/", files[grep(pattern = paste0(site, ".*.", date,            ".*.h5.gz"), x = files)]), remove = FALSE)
  files <- list.files(paste0(DirDnld, "/filesToStack00200"))
  dataIdx <- rhdf5::h5read(file = paste0(DirDnld, "/filesToStack00200/", max(files[grep(pattern =                  paste0(site,".*.", date,".*.h5$"), x = files)])), name = paste0(site, "/"))
  
  if(!is.null(dataIdx)){ 
    dataIdx$dp0p <- NULL 
    dataIdx$dp02 <- NULL 
    dataIdx$dp03 <- NULL
    dataIdx$dp01$ucrt <- NULL 
    dataIdx$dp04$ucrt <- NULL 
    dataIdx$dp01$data <- lapply(dataIdx$dp01$data,FUN=function(var){ 
      nameTmi <- names(var) 
      var <- var[grepl('_30m',nameTmi)] 
      return(var)})
    dataIdx$dp01$qfqm <- lapply(dataIdx$dp01$qfqm,FUN=function(var){ 
      nameTmi <- names(var)
      var <- var[grepl('_30m',nameTmi)]
      return(var)})
  }
  return(dataIdx)
})


# Add names to list for year/month combinations
names(dataList) <- paste0(lubridate::year(setDate),sprintf("%02d",lubridate::month(setDate)))

# Remove NULL elements from list
dataList <- dataList[vapply(dataList, Negate(is.null), NA)]

# Determine tower horizontal & vertical indices

#Find the tower top level by looking at the vertical index of the turbulent CO2 concentration measurements 
LvlTowr <- grep(pattern = "_30m", names(dataList[[1]]$dp01$data$co2Turb), value = TRUE)
LvlTowr <- gsub(x = LvlTowr, pattern = "_30m", replacement = "")

#get tower top level
LvlTop <- strsplit(LvlTowr,"")
LvlTop <- base::as.numeric(LvlTop[[1]][6])

#Ameriflux vertical levels based off of https://ameriflux.lbl.gov/data/aboutdata/data-variables/ section 3.3.1 "Indices must be in order, starting with the highest."
idxVerAmfx <- base::seq(from = 1, to = LvlTop, by = 1)
#get the sequence from top to first level
LvlMeas <- base::seq(from = LvlTop, to = 1, by = -1)
#Recreate NEON naming conventions
LvlMeas <- paste0("000_0",LvlMeas,"0",sep="")
#Give NEON naming conventions to Ameriflux vertical levels
names(idxVerAmfx) <- LvlMeas

#Ameriflux horizontal index
idxHorAmfx <- 1

# Subset to the Ameriflux variables to convert
dataListFlux <- lapply(names(dataList), function(x) {
  data.frame(
    "TIMESTAMP_START" = as.POSIXlt(dataList[[x]]$dp04$data$fluxCo2$turb$timeBgn, format="%Y-%m-%dT%H:%M:%OSZ", tz = "GMT"),
    "TIMESTAMP_END" = as.POSIXlt(dataList[[x]]$dp04$data$fluxCo2$turb$timeEnd, format="%Y-%m-%dT%H:%M:%OSZ", tz = "GMT"),
    # "TIMESTAMP_START" = strftime(as.POSIXlt(dataList[[x]][[idxSite]]$dp04$data$fluxCo2$turb$timeBgn, format="%Y-%m-%dT%H:%M:%OSZ"), format = "%Y%m%d%H%M"),
    # "TIMESTAMP_END" = strftime(as.POSIXlt(dataList[[x]][[idxSite]]$dp04$data$fluxCo2$turb$timeEnd, format="%Y-%m-%dT%H:%M:%OSZ") + 60, format = "%Y%m%d%H%M"),
    "FC"= dataList[[x]]$dp04$data$fluxCo2$turb$flux,
    "SC"= dataList[[x]]$dp04$data$fluxCo2$stor$flux,
    "NEE"= dataList[[x]]$dp04$data$fluxCo2$nsae$flux,
    "LE" = dataList[[x]]$dp04$data$fluxH2o$turb$flux,
    "SLE" = dataList[[x]]$dp04$data$fluxH2o$stor$flux,
    "USTAR" = dataList[[x]]$dp04$data$fluxMome$turb$veloFric,
    
    "H" = dataList[[x]]$dp04$data$fluxTemp$turb$flux,
    "SH" = dataList[[x]]$dp04$data$fluxTemp$stor$flux,
    "FETCH_90" = dataList[[x]]$dp04$data$foot$stat$distXaxs90,
    "FETCH_MAX" = dataList[[x]]$dp04$data$foot$stat$distXaxsMax,
    "V_SIGMA" = dataList[[x]]$dp04$data$foot$stat$veloYaxsHorSd,
    #"W_SIGMA" = dataList[[x]]$dp04$data$foot$stat$veloZaxsHorSd,
    "CO2_1_1_1" = dataList[[x]]$dp01$data$co2Turb[[paste0(LvlTowr,"_30m")]]$rtioMoleDryCo2$mean,
    "H2O_1_1_1" = dataList[[x]]$dp01$data$h2oTurb[[paste0(LvlTowr,"_30m")]]$rtioMoleDryH2o$mean,
    "qfFinlH2oTurbFrt00Samp" = dataList[[x]]$dp01$qfqm$h2oTurb[[paste0(LvlTowr,"_30m")]]$frt00Samp$qfFinl,
    "qfH2O_1_1_1" = dataList[[x]]$dp01$qfqm$h2oTurb[[paste0(LvlTowr,"_30m")]]$rtioMoleDryH2o$qfFinl,
    "qfCO2_1_1_1" = dataList[[x]]$dp01$qfqm$co2Turb[[paste0(LvlTowr,"_30m")]]$rtioMoleDryCo2$qfFinl,
    "qfSC" = dataList[[x]]$dp04$qfqm$fluxCo2$stor$qfFinl,
    "qfSLE" = dataList[[x]]$dp04$qfqm$fluxH2o$stor$qfFinl,
    "qfSH" = dataList[[x]]$dp04$qfqm$fluxTemp$stor$qfFinl,
    "qfT_SONIC" = dataList[[x]]$dp01$qfqm$soni[[paste0(LvlTowr,"_30m")]]$tempSoni$qfFinl,
    "qfWS_1_1_1" = dataList[[x]]$dp01$qfqm$soni[[paste0(LvlTowr,"_30m")]]$veloXaxsYaxsErth$qfFinl,
    rbind.data.frame(lapply(names(idxVerAmfx), function(y) {
      tryCatch({rlog$debug(y)}, error=function(cond){print(y)})
      rpt <- list()
      rpt[[paste0("CO2_1_",idxVerAmfx[y],"_2")]] <- dataList[[x]]$dp01$data$co2Stor[[paste0(y,"_30m")]]$rtioMoleDryCo2$mean
      
      
      rpt[[paste0("H2O_1_",idxVerAmfx[y],"_2")]] <- dataList[[x]]$dp01$data$h2oStor[[paste0(y,"_30m")]]$rtioMoleDryH2o$mean
      rpt[[paste0("CO2_1_",idxVerAmfx[y],"_3")]] <- dataList[[x]]$dp01$data$isoCo2[[paste0(y,"_30m")]]$rtioMoleDryCo2$mean
      
      rpt[[paste0("H2O_1_",idxVerAmfx[y],"_3")]] <- dataList[[x]]$dp01$data$isoCo2[[paste0(y,"_30m")]]$rtioMoleDryH2o$mean
      rpt[[paste0("qfCO2_1_",idxVerAmfx[y],"_2")]] <- dataList[[x]]$dp01$qfqm$co2Stor[[paste0(LvlTowr,"_30m")]]$rtioMoleDryCo2$qfFinl
      rpt[[paste0("qfH2O_1_",idxVerAmfx[y],"_2")]] <- dataList[[x]]$dp01$qfqm$h2oStor[[paste0(LvlTowr,"_30m")]]$rtioMoleDryH2o$qfFinl
      rpt[[paste0("qfCO2_1_",idxVerAmfx[y],"_3")]] <- dataList[[x]]$dp01$qfqm$isoCo2[[paste0(LvlTowr,"_30m")]]$rtioMoleDryCo2$qfFinl
      rpt[[paste0("qfH2O_1_",idxVerAmfx[y],"_3")]] <- dataList[[x]]$dp01$qfqm$isoH2o[[paste0(LvlTowr,"_30m")]]$rtioMoleDryH2o$qfFinl
      
      rpt <- rbind.data.frame(rpt)
      return(rpt)
    }
    )),
    
    
    "WS_1_1_1" = dataList[[x]]$dp01$data$soni[[paste0(LvlTowr,"_30m")]]$veloXaxsYaxsErth$mean,
    "WS_MAX_1_1_1" = dataList[[x]]$dp01$data$soni[[paste0(LvlTowr,"_30m")]]$veloXaxsYaxsErth$max,
    "WD_1_1_1" = dataList[[x]]$dp01$data$soni[[paste0(LvlTowr,"_30m")]]$angZaxsErth$mean,
    "T_SONIC" = dataList[[x]]$dp01$data$soni[[paste0(LvlTowr,"_30m")]]$tempSoni$mean,
    "T_SONIC_SIGMA" = base::sqrt(dataList[[x]]$dp01$data$soni[[paste0(LvlTowr,"_30m")]]$tempSoni$mean)
    , stringsAsFactors = FALSE)
})

names(dataListFlux) <- names(dataList)

# Combine the monthly data into a single dataframe, remove lists and clean memory
dataDfFlux <- do.call(rbind.data.frame,dataListFlux)
rm(list=c("dataListFlux","dataList"))
gc()

# Regularize timeseries to 30 minutes in case timestamps are missing from NEON files due to processing errors

timeRglr <- eddy4R.base::def.rglr(timeMeas = as.POSIXlt(dataDfFlux$TIMESTAMP_START), dataMeas = dataDfFlux, BgnRglr = as.POSIXlt(dataDfFlux$TIMESTAMP_START[1]), EndRglr = as.POSIXlt(dataDfFlux$TIMESTAMP_END[length(dataDfFlux$TIMESTAMP_END)]), TzRglr = "UTC", FreqRglr = 1/(60*30))

#Reassign data to data.frame
dataDfFlux <- timeRglr$dataRglr
#Format timestamps
dataDfFlux$TIMESTAMP_START <- strftime(timeRglr$timeRglr + lubridate::hours(timeOfstUtc), format = "%Y%m%d%H%M")
dataDfFlux$TIMESTAMP_END <- strftime(timeRglr$timeRglr + lubridate::hours(timeOfstUtc) + lubridate::minutes(30), format = "%Y%m%d%H%M")

# Define validation times, and remove this data from the dataset. At NEON sites, validations with a series of gasses of known concentration are run every 23.5 hours. These values are used to correct for measurment drift and are run every 23.5 hours to achive daily resolution while also spreading the impact of lost measurements throughout the day.

#Remove co2Turb and h2oTurb data based off of qfFlow (qfFinl frt00)
dataDfFlux$FC[(which(dataDfFlux$qfCO2_1_1_1 == 1))] <- NaN
dataDfFlux$LE[(which(dataDfFlux$qfH2O_1_1_1 == 1))] <- NaN
dataDfFlux$USTAR[(which(dataDfFlux$qfWS_1_1_1 == 1))] <- NaN
dataDfFlux$H[(which(dataDfFlux$qfT_SONIC_1_1_1 == 1))] <- NaN
dataDfFlux$SC[(which(dataDfFlux$qfSC == 1))] <- NaN
dataDfFlux$SLE[(which(dataDfFlux$qfSLE == 1))] <- NaN
dataDfFlux$SH[(which(dataDfFlux$qfSH == 1))] <- NaN
dataDfFlux$T_SONIC[(which(dataDfFlux$qfT_SONIC_1_1_1 == 1))] <- NaN
dataDfFlux$T_SONIC_SIGMA[(which(dataDfFlux$qfT_SONIC_1_1_1 == 1))] <- NaN
dataDfFlux$WS_1_1_1[(which(dataDfFlux$qfWS_1_1_1 == 1))] <- NaN
dataDfFlux$WS_MAX_1_1_1[(which(dataDfFlux$qfWS_1_1_1 == 1))] <- NaN
dataDfFlux$WD_1_1_1[(which(dataDfFlux$qfWS_1_1_1 == 1))] <- NaN

dataDfFlux$H2O_1_1_1[(which(dataDfFlux$qfH2O_1_1_1 == 1))] <- NaN
dataDfFlux$CO2_1_1_1[(which(dataDfFlux$qfCO2_1_1_1 == 1))] <- NaN

lapply(idxVerAmfx, function(x){
  #x <- 1
  dataDfFlux[[paste0("H2O_1_",x,"_2")]][(which(dataDfFlux[[paste0("qfH2O_1_",x,"_2")]] == 1))] <<- NaN
  dataDfFlux[[paste0("H2O_1_",x,"_3")]][(which(dataDfFlux[[paste0("qfH2O_1_",x,"_3")]] == 1))] <<- NaN
  dataDfFlux[[paste0("CO2_1_",x,"_2")]][(which(dataDfFlux[[paste0("qfCO2_1_",x,"_2")]] == 1))] <<- NaN
  dataDfFlux[[paste0("CO2_1_",x,"_3")]][(which(dataDfFlux[[paste0("qfCO2_1_",x,"_3")]] == 1))] <<- NaN
})


```

```{r}

```


## Question 2
**Using metScanR package, find co-located NEON and AmeriFlux sites. Download data for an overlapping time period, and compare FC and H values by making a scatter plot and seeing how far off the data are from a 1:1 line.**