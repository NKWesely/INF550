---
title: "INF 550 Section 5.7"
author: "Natasha Wesely"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(neonUtilities)
# install.packages('BiocManager')
library(BiocManager)
# BiocManager::install('rhdf5')
library(rhdf5)
library(eddy4R.base)
library(jsonlite)
library(lubridate)

options(stringsAsFactors=F)

# Ameriflux site that relates to JORN
# US-xJR
```

# Flux Measurements & Inter-Operability Coding Exercise
## Question 1
**NEON data are submitted to AmeriFlux quarterly after one year of non-quality flagged or otherwise missing data are available. Use the workflow above to extend the data coverage of an already submitted NEON site by downloading existing data from the AmeriFlux website and recently published HDF5 files from the NEON data portal. Process the NEON data such that it is in AmeriFlux format and plot the entire timerseries.**

```{r}

# # download NEON data for JORN site
# zipsByProduct(dpID="DP4.00200.001", 
#               package="basic", 
#               site="JORN", 
#               startdate="2018-06", 
#               enddate="2018-07",
#               savepath="./data_5.7", 
#               check.size=F)
# 
# # download recently published HDF5 filse from NEON data portal 
# 
# 
# flux <- stackEddy(filepath="./data_5.7/filesToStack00200",
#                   level="dp04")
# 
# # fix the dates
# timeB <- as.POSIXct(flux$NIWO$timeBgn, 
#                     format="%Y-%m-%dT%H:%M:%S", 
#                     tz="GMT")
# flux$NIWO <- cbind(timeB, flux$NIWO)
# 
# 
# # download another data product 
# pr <- loadByProduct("DP1.00024.001", site="NIWO", avg=30,
#                     startdate="2018-06", enddate="2018-07",
#                     package="basic", check.size=F)

```

```{r}
# #Install most  recent version of ffbase from GitHub, this is a package dependency of eddy4R.base
# devtools::install_github("edwindj/ffbase", subdir="pkg")
# 
# #Install NEONprocIS.base from GitHub, this package is a dependency of eddy4R.base
# devtools::install_github(repo="NEONScience/NEON-IS-data-processing",
#                          ref="master",
#                          subdir="pack/NEONprocIS.base",
#                          dependencies=c(NA, TRUE)[2],
#                          repos=c(BiocManager::repositories(),   # for dependencies on Bioconductor packages
#                                  "https://cran.rstudio.com/")       # for CRAN
# )
# 
# #Install eddy4R.base from GitHub
# devtools::install_github(repo="NEONScience/eddy4R",
#                          ref="master",
#                          subdir="pack/eddy4R.base",
#                          dependencies=c(NA, TRUE)[2],
#                          repos=c(BiocManager::repositories(),   # for dependencies on Bioconductor packages
#                                  "https://cran.rstudio.com/")       # for CRAN
# )

# # make a list of the required packages
# packReq <- c("rhdf5", "eddy4R.base", "jsonlite", "lubridate")
# 
# # load the required packages
# lapply(packReq, function(x) {
#   print(x)
#   if(require(x, character.only = TRUE) == FALSE) {
#     install.packages(x)
#     library(x, character.only = TRUE)
#   }})

# disable file locking 
h5disableFileLocking()

# choose your NEON site of interest
site <- "JORN"

#define start and end dates, optional, defaults to entire period of site operation. Use %Y-%m-%d format.
dateBgn <- "2021-01-01"
dateEnd <- "2021-12-31"

# Data package from the portal
Pack <- c('basic','expanded')[1]
#The version data for the FP standard conversion processing
ver = paste0("v",format(Sys.time(), "%Y%m%dT%H%m"))


# Specify Download directory for HDF5 files from the NEON data portal and output directory to save the resulting csv files. Change save paths to where you want the files on your computer.
#download directory
DirDnld=tempdir()

#Output directory, change this to where you want to save the output csv
DirOutBase <-paste0("~/data_5.7/eddy/data/Ameriflux/",ver)

# Specify Data Product number, for the Bundled Eddy-Covariance files, this is DP4.00200.001
#DP number
dpID <- 'DP4.00200.001'


# Get metadata from Ameriflux Site Info BADM sheets for the site of interest

#Grab a list of all Ameriflux sites, containing site ID and site description
sites_web <- jsonlite::fromJSON("http://ameriflux-data.lbl.gov/AmeriFlux/SiteSearch.svc/SiteList/AmeriFlux")

#Grab only your neon site of interest NEON site from the AmeriFlux sites
siteNeon <- sites_web[grep(pattern = paste0("NEON.*",site), x = sites_web$SITE_NAME),]

# get the AmeriFlux site info for your site of interest
metaSite <- lapply(siteNeon$SITE_ID, function(x) {
  pathSite <- paste0("http://ameriflux-data.lbl.gov/BADM/Anc/SiteInfo/",x)
  tmp <- fromJSON(pathSite)
  return(tmp)
}) 


# Use Ameriflux site IDs to name metadata lists
#use NEON ID as list name
names(metaSite) <- site 

# Check if dateBgn is defined, if not make it the initial operations date “IOCR” of the site
if(!exists("dateBgn") || is.na(dateBgn) || is.null(dateBgn)){
  dateBgn <- as.Date(metaSite[[site]]$values$GRP_FLUX_MEASUREMENTS[[1]]$FLUX_MEASUREMENTS_DATE_START, "%Y%m%d")
} else {
  dateBgn <- dateBgn
}#End of checks for missing dateBgn

#Check if dateEnd is defined, if not make it the system date
if(!exists("dateEnd") || is.na(dateEnd) || is.null(dateEnd)){
  dateEnd <- as.Date(Sys.Date())
} else {
  dateEnd <- dateEnd
}#End of checks for missing dateEnd


# Grab the UTC time offset from the Ameriflux API
timeOfstUtc <- as.integer(metaSite[[site]]$values$GRP_UTC_OFFSET[[1]]$UTC_OFFSET)

# Create the date sequence
setDate <- seq(from = as.Date(dateBgn), to = as.Date(dateEnd), by = "month")

# Start processing the site time range specified, verify that the site and date range are specified as intended
msg <- paste0("Starting Ameriflux FP standard conversion processing workflow for ", site, " for ", dateBgn, " to ", dateEnd)
print(msg)


# Create output directory by checking if the download directory exists and create it if not
if(dir.exists(DirDnld) == FALSE) dir.create(DirDnld, recursive = TRUE)
#Append the site to the base output directory
DirOut <- paste0(DirOutBase, "/", siteNeon$SITE_ID)
#Check if directory exists and create if not
if(!dir.exists(DirOut)) dir.create(DirOut, recursive = TRUE)

# Download and extract data

#Initialize data List
dataList <- list()

#Read data from the API
dataList <- lapply(setDate, function(x) {
  date <- stringr::str_extract(x, pattern = paste0("[0-9]{4}", "-", "[0-9]{2}"))
  tryCatch(neonUtilities::zipsByProduct(dpID = dpID, site = site, startdate = date, enddate = date, package =      "basic", savepath = DirDnld, check.size = FALSE), error=function(e) NULL)
  files <- list.files(paste0(DirDnld, "/filesToStack00200"))
  utils::unzip(paste0(DirDnld, "/filesToStack00200/", files[grep(pattern = paste0(site,".*.", date, ".*.zip"),     x = files)]), exdir = paste0(DirDnld, "/filesToStack00200"))
  files <- list.files(paste0(DirDnld, "/filesToStack00200"))
  R.utils::gunzip(paste0(DirDnld, "/filesToStack00200/", files[grep(pattern = paste0(site, ".*.", date,            ".*.h5.gz"), x = files)]), remove = FALSE)
  files <- list.files(paste0(DirDnld, "/filesToStack00200"))
  dataIdx <- rhdf5::h5read(file = paste0(DirDnld, "/filesToStack00200/", max(files[grep(pattern =                  paste0(site,".*.", date,".*.h5$"), x = files)])), name = paste0(site, "/"))
  
  if(!is.null(dataIdx)){ 
    dataIdx$dp0p <- NULL 
    dataIdx$dp02 <- NULL 
    dataIdx$dp03 <- NULL
    dataIdx$dp01$ucrt <- NULL 
    dataIdx$dp04$ucrt <- NULL 
    dataIdx$dp01$data <- lapply(dataIdx$dp01$data,FUN=function(var){ 
      nameTmi <- names(var) 
      var <- var[grepl('_30m',nameTmi)] 
      return(var)})
    dataIdx$dp01$qfqm <- lapply(dataIdx$dp01$qfqm,FUN=function(var){ 
      nameTmi <- names(var)
      var <- var[grepl('_30m',nameTmi)]
      return(var)})
  }
  return(dataIdx)
})


# Add names to list for year/month combinations
names(dataList) <- paste0(lubridate::year(setDate),sprintf("%02d",lubridate::month(setDate)))

# Remove NULL elements from list
dataList <- dataList[vapply(dataList, Negate(is.null), NA)]
```

```{r}

```


## Question 2
**Using metScanR package, find co-located NEON and AmeriFlux sites. Download data for an overlapping time period, and compare FC and H values by making a scatter plot and seeing how far off the data are from a 1:1 line.**